{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cleverhans\n",
    "%matplotlib inline\n",
    "from cleverhans.model import Model\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henripal/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sgld_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Model):\n",
    "    def __init__(self):\n",
    "        self.layer_names = ['reshape',\n",
    "                           'conv1',\n",
    "                           'pool1',\n",
    "                           'conv2',\n",
    "                           'pool2',\n",
    "                           'pool2_flat',\n",
    "                           'dense',\n",
    "                           'logits',\n",
    "                           'probs']\n",
    "    \n",
    "    def fprop(self, x):\n",
    "        reshaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        conv1, conv1_pre, conv1_out = self.make_conv_layer(32, [5, 5], reshaped_input)   \n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1_out, pool_size=[2, 2], strides=2)\n",
    "\n",
    "        conv2, conv2_pre, conv2_out = self.make_conv_layer(64, [5, 5], pool1)         \n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2_out, pool_size=[2, 2], strides=2)\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])                              \n",
    "\n",
    "        dense, dense_pre, dense_out = self.make_dense_layer(1024, pool2_flat)         \n",
    "        dense2, logits, dense2_out = self.make_dense_layer(10, dense_out)             \n",
    "        probs = tf.nn.softmax(logits)\n",
    "        \n",
    "        return {'reshape': reshaped_input,\n",
    "               'conv1': conv1_out,\n",
    "               'pool1': pool1,\n",
    "               'conv2': conv2_out,\n",
    "               'pool2': pool2,\n",
    "               'pool2_flat': pool2_flat,\n",
    "               'dense': dense_out,\n",
    "               'logits': logits,\n",
    "               'probs': probs}\n",
    "        \n",
    "    def make_conv_layer(self, n_filters, kernel_size, inputs, padding=\"same\"):\n",
    "        conv = tf.layers.Conv2D(filters=n_filters,                             \n",
    "                              kernel_size=kernel_size,                         \n",
    "                              padding=padding)                                 \n",
    "        pre = conv(inputs)                                                     \n",
    "        out = tf.nn.relu(pre)                                                  \n",
    "\n",
    "        return conv, pre, out                                                  \n",
    "\n",
    "    def make_dense_layer(self, n_units, inputs):                               \n",
    "        dense = tf.layers.Dense(units=n_units)                                 \n",
    "        pre = dense(inputs)                                                    \n",
    "        out = tf.nn.relu(pre)                                                  \n",
    "\n",
    "        return dense, pre, out                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleverhans.utils_mnist import data_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/henripal/projects/cleverhans/cleverhans/utils_mnist.py:31: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/henripal/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data_mnist(train_start=0,\n",
    "                                                  train_end=60000,\n",
    "                                                  test_start=0,\n",
    "                                                  test_end=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model.fprop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model_dict['probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = model_dict['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(y, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = sgld_tf.SGLD(learning_rate = 0.01, lrdecay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_step = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## saver.save(sess, path)\n",
    "correct_preds = tf.equal(tf.argmax(y, axis=-1), tf.argmax(preds, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "batch loss:  2.31034\n",
      "batch loss:  2.31022\n",
      "batch loss:  2.30413\n",
      "batch loss:  2.30159\n",
      "batch loss:  2.29983\n",
      "batch loss:  2.30118\n",
      "batch loss:  2.30262\n",
      "batch loss:  2.30622\n",
      "batch loss:  2.29781\n",
      "batch loss:  2.29579\n",
      "batch loss:  2.29489\n",
      "batch loss:  2.29645\n",
      "batch loss:  2.28804\n",
      "batch loss:  2.29556\n",
      "batch loss:  2.29499\n",
      "batch loss:  2.2899\n",
      "batch loss:  2.29166\n",
      "batch loss:  2.29178\n",
      "batch loss:  2.29473\n",
      "batch loss:  2.28787\n",
      "batch loss:  2.28719\n",
      "batch loss:  2.28379\n",
      "batch loss:  2.2829\n",
      "batch loss:  2.29052\n",
      "batch loss:  2.28562\n",
      "batch loss:  2.28314\n",
      "batch loss:  2.28163\n",
      "batch loss:  2.28169\n",
      "batch loss:  2.27426\n",
      "batch loss:  2.27569\n",
      "batch loss:  2.27667\n",
      "batch loss:  2.27864\n",
      "batch loss:  2.27216\n",
      "batch loss:  2.27349\n",
      "batch loss:  2.27486\n",
      "batch loss:  2.26946\n",
      "batch loss:  2.27399\n",
      "batch loss:  2.27372\n",
      "batch loss:  2.27455\n",
      "batch loss:  2.26534\n",
      "batch loss:  2.26401\n",
      "batch loss:  2.26727\n",
      "batch loss:  2.26679\n",
      "batch loss:  2.2628\n",
      "batch loss:  2.2609\n",
      "batch loss:  2.25583\n",
      "batch loss:  2.26434\n",
      "batch loss:  2.25723\n",
      "batch loss:  2.26463\n",
      "batch loss:  2.26286\n",
      "batch loss:  2.26248\n",
      "batch loss:  2.25924\n",
      "batch loss:  2.25965\n",
      "batch loss:  2.26411\n",
      "batch loss:  2.25433\n",
      "batch loss:  2.25271\n",
      "batch loss:  2.25264\n",
      "batch loss:  2.24929\n",
      "batch loss:  2.25663\n",
      "batch loss:  2.25048\n",
      "batch loss:  2.24809\n",
      "batch loss:  2.24987\n",
      "batch loss:  2.24413\n",
      "batch loss:  2.24522\n",
      "batch loss:  2.24321\n",
      "batch loss:  2.24446\n",
      "batch loss:  2.23718\n",
      "batch loss:  2.24099\n",
      "batch loss:  2.2346\n",
      "batch loss:  2.23784\n",
      "batch loss:  2.23603\n",
      "batch loss:  2.24241\n",
      "batch loss:  2.23835\n",
      "batch loss:  2.23237\n",
      "batch loss:  2.23516\n",
      "batch loss:  2.23286\n",
      "batch loss:  2.23999\n",
      "batch loss:  2.22195\n",
      "batch loss:  2.22591\n",
      "batch loss:  2.22732\n",
      "batch loss:  2.23125\n",
      "batch loss:  2.2143\n",
      "batch loss:  2.21968\n",
      "batch loss:  2.21853\n",
      "batch loss:  2.21028\n",
      "batch loss:  2.21642\n",
      "batch loss:  2.22141\n",
      "batch loss:  2.2246\n",
      "batch loss:  2.21313\n",
      "batch loss:  2.21355\n",
      "batch loss:  2.20941\n",
      "batch loss:  2.21058\n",
      "batch loss:  2.21018\n",
      "batch loss:  2.21323\n",
      "batch loss:  2.20405\n",
      "batch loss:  2.20386\n",
      "batch loss:  2.19819\n",
      "batch loss:  2.19789\n",
      "batch loss:  2.19218\n",
      "batch loss:  2.19493\n",
      "batch loss:  2.18844\n",
      "batch loss:  2.1874\n",
      "batch loss:  2.19889\n",
      "batch loss:  2.18365\n",
      "batch loss:  2.18802\n",
      "batch loss:  2.17643\n",
      "batch loss:  2.18251\n",
      "batch loss:  2.18204\n",
      "batch loss:  2.18972\n",
      "batch loss:  2.19824\n",
      "batch loss:  2.17873\n",
      "batch loss:  2.16924\n",
      "batch loss:  2.17797\n",
      "batch loss:  2.17904\n",
      "batch loss:  2.16391\n",
      "batch loss:  2.16866\n",
      "batch loss:  2.15764\n",
      "batch loss:  2.16753\n",
      "eval_acc  0.5349\n",
      "epoch:  1\n",
      "batch loss:  2.15569\n",
      "batch loss:  2.16825\n",
      "batch loss:  2.14995\n",
      "batch loss:  2.1557\n",
      "batch loss:  2.17134\n",
      "batch loss:  2.15283\n",
      "batch loss:  2.1545\n",
      "batch loss:  2.15222\n",
      "batch loss:  2.14359\n",
      "batch loss:  2.12645\n",
      "batch loss:  2.13274\n",
      "batch loss:  2.13316\n",
      "batch loss:  2.13462\n",
      "batch loss:  2.14074\n",
      "batch loss:  2.14126\n",
      "batch loss:  2.13591\n",
      "batch loss:  2.12799\n",
      "batch loss:  2.13096\n",
      "batch loss:  2.14197\n",
      "batch loss:  2.12755\n",
      "batch loss:  2.11792\n",
      "batch loss:  2.12585\n",
      "batch loss:  2.11016\n",
      "batch loss:  2.12721\n",
      "batch loss:  2.13104\n",
      "batch loss:  2.10473\n",
      "batch loss:  2.11007\n",
      "batch loss:  2.10632\n",
      "batch loss:  2.078\n",
      "batch loss:  2.09322\n",
      "batch loss:  2.08898\n",
      "batch loss:  2.08556\n",
      "batch loss:  2.07721\n",
      "batch loss:  2.07035\n",
      "batch loss:  2.09021\n",
      "batch loss:  2.05705\n",
      "batch loss:  2.0854\n",
      "batch loss:  2.07488\n",
      "batch loss:  2.08687\n",
      "batch loss:  2.04406\n",
      "batch loss:  2.04891\n",
      "batch loss:  2.05488\n",
      "batch loss:  2.05886\n",
      "batch loss:  2.04605\n",
      "batch loss:  2.03392\n",
      "batch loss:  2.01308\n",
      "batch loss:  2.03799\n",
      "batch loss:  2.02295\n",
      "batch loss:  2.06053\n",
      "batch loss:  2.04561\n",
      "batch loss:  2.04483\n",
      "batch loss:  2.04013\n",
      "batch loss:  2.03903\n",
      "batch loss:  2.05109\n",
      "batch loss:  2.00948\n",
      "batch loss:  2.0085\n",
      "batch loss:  1.9897\n",
      "batch loss:  1.97778\n",
      "batch loss:  2.01629\n",
      "batch loss:  1.98849\n",
      "batch loss:  1.97957\n",
      "batch loss:  1.96771\n",
      "batch loss:  1.96797\n",
      "batch loss:  1.9683\n",
      "batch loss:  1.96327\n",
      "batch loss:  1.96748\n",
      "batch loss:  1.92205\n",
      "batch loss:  1.96169\n",
      "batch loss:  1.9436\n",
      "batch loss:  1.93807\n",
      "batch loss:  1.9378\n",
      "batch loss:  1.96692\n",
      "batch loss:  1.94058\n",
      "batch loss:  1.91638\n",
      "batch loss:  1.9272\n",
      "batch loss:  1.90933\n",
      "batch loss:  1.94256\n",
      "batch loss:  1.88054\n",
      "batch loss:  1.88473\n",
      "batch loss:  1.89462\n",
      "batch loss:  1.9085\n",
      "batch loss:  1.8328\n",
      "batch loss:  1.86538\n",
      "batch loss:  1.85264\n",
      "batch loss:  1.79126\n",
      "batch loss:  1.83095\n",
      "batch loss:  1.8556\n",
      "batch loss:  1.88315\n",
      "batch loss:  1.82669\n",
      "batch loss:  1.82266\n",
      "batch loss:  1.79425\n",
      "batch loss:  1.81517\n",
      "batch loss:  1.7996\n",
      "batch loss:  1.81649\n",
      "batch loss:  1.77814\n",
      "batch loss:  1.78897\n",
      "batch loss:  1.74272\n",
      "batch loss:  1.74211\n",
      "batch loss:  1.713\n",
      "batch loss:  1.72062\n",
      "batch loss:  1.71029\n",
      "batch loss:  1.70113\n",
      "batch loss:  1.7537\n",
      "batch loss:  1.67966\n",
      "batch loss:  1.69523\n",
      "batch loss:  1.62028\n",
      "batch loss:  1.65831\n",
      "batch loss:  1.65344\n",
      "batch loss:  1.6951\n",
      "batch loss:  1.71098\n",
      "batch loss:  1.63803\n",
      "batch loss:  1.5902\n",
      "batch loss:  1.62237\n",
      "batch loss:  1.64544\n",
      "batch loss:  1.59116\n",
      "batch loss:  1.60588\n",
      "batch loss:  1.54447\n",
      "batch loss:  1.59769\n",
      "eval_acc  0.7265\n",
      "epoch:  2\n",
      "batch loss:  1.54382\n",
      "batch loss:  1.5867\n",
      "batch loss:  1.50113\n",
      "batch loss:  1.55288\n",
      "batch loss:  1.63206\n",
      "batch loss:  1.54076\n",
      "batch loss:  1.52584\n",
      "batch loss:  1.51121\n",
      "batch loss:  1.48295\n",
      "batch loss:  1.43138\n",
      "batch loss:  1.42277\n",
      "batch loss:  1.43112\n",
      "batch loss:  1.44774\n",
      "batch loss:  1.46312\n",
      "batch loss:  1.48075\n",
      "batch loss:  1.45957\n",
      "batch loss:  1.40935\n",
      "batch loss:  1.44277\n",
      "batch loss:  1.48431\n",
      "batch loss:  1.40424\n",
      "batch loss:  1.38483\n",
      "batch loss:  1.43179\n",
      "batch loss:  1.34487\n",
      "batch loss:  1.42898\n",
      "batch loss:  1.44782\n",
      "batch loss:  1.31178\n",
      "batch loss:  1.34505\n",
      "batch loss:  1.33894\n",
      "batch loss:  1.22562\n",
      "batch loss:  1.30437\n",
      "batch loss:  1.29206\n",
      "batch loss:  1.27689\n",
      "batch loss:  1.24315\n",
      "batch loss:  1.22902\n",
      "batch loss:  1.28982\n",
      "batch loss:  1.17704\n",
      "batch loss:  1.25874\n",
      "batch loss:  1.23387\n",
      "batch loss:  1.29194\n",
      "batch loss:  1.13798\n",
      "batch loss:  1.15723\n",
      "batch loss:  1.18221\n",
      "batch loss:  1.21073\n",
      "batch loss:  1.19039\n",
      "batch loss:  1.11728\n",
      "batch loss:  1.07503\n",
      "batch loss:  1.14254\n",
      "batch loss:  1.11281\n",
      "batch loss:  1.24906\n",
      "batch loss:  1.19714\n",
      "batch loss:  1.19102\n",
      "batch loss:  1.19702\n",
      "batch loss:  1.14948\n",
      "batch loss:  1.21129\n",
      "batch loss:  1.10085\n",
      "batch loss:  1.07837\n",
      "batch loss:  1.01149\n",
      "batch loss:  1.02411\n",
      "batch loss:  1.12147\n",
      "batch loss:  1.04348\n",
      "batch loss:  1.0367\n",
      "batch loss:  0.951303\n",
      "batch loss:  1.01437\n",
      "batch loss:  1.06894\n",
      "batch loss:  1.0121\n",
      "batch loss:  1.00871\n",
      "batch loss:  0.894899\n",
      "batch loss:  1.0667\n",
      "batch loss:  0.988544\n",
      "batch loss:  0.960316\n",
      "batch loss:  0.991172\n",
      "batch loss:  1.06139\n",
      "batch loss:  1.0147\n",
      "batch loss:  0.944666\n",
      "batch loss:  0.981876\n",
      "batch loss:  0.910319\n",
      "batch loss:  1.0267\n",
      "batch loss:  0.881009\n",
      "batch loss:  0.917303\n",
      "batch loss:  0.947248\n",
      "batch loss:  0.969801\n",
      "batch loss:  0.812333\n",
      "batch loss:  0.900845\n",
      "batch loss:  0.883573\n",
      "batch loss:  0.760641\n",
      "batch loss:  0.870968\n",
      "batch loss:  0.91531\n",
      "batch loss:  0.963803\n",
      "batch loss:  0.903478\n",
      "batch loss:  0.8426\n",
      "batch loss:  0.810299\n",
      "batch loss:  0.852695\n",
      "batch loss:  0.841435\n",
      "batch loss:  0.929056\n",
      "batch loss:  0.816171\n",
      "batch loss:  0.840743\n",
      "batch loss:  0.765514\n",
      "batch loss:  0.798518\n",
      "batch loss:  0.740898\n",
      "batch loss:  0.738088\n",
      "batch loss:  0.760366\n",
      "batch loss:  0.737398\n",
      "batch loss:  0.85937\n",
      "batch loss:  0.696096\n",
      "batch loss:  0.693275\n",
      "batch loss:  0.571219\n",
      "batch loss:  0.722059\n",
      "batch loss:  0.711026\n",
      "batch loss:  0.775553\n",
      "batch loss:  0.865848\n",
      "batch loss:  0.726086\n",
      "batch loss:  0.66964\n",
      "batch loss:  0.736199\n",
      "batch loss:  0.76646\n",
      "batch loss:  0.718641\n",
      "batch loss:  0.727828\n",
      "batch loss:  0.674078\n",
      "batch loss:  0.730942\n",
      "eval_acc  0.8095\n",
      "epoch:  3\n",
      "batch loss:  0.709429\n",
      "batch loss:  0.72591\n",
      "batch loss:  0.621867\n",
      "batch loss:  0.723814\n",
      "batch loss:  0.818267\n",
      "batch loss:  0.749794\n",
      "batch loss:  0.735098\n",
      "batch loss:  0.741724\n",
      "batch loss:  0.690095\n",
      "batch loss:  0.614165\n",
      "batch loss:  0.589935\n",
      "batch loss:  0.638932\n",
      "batch loss:  0.687095\n",
      "batch loss:  0.683139\n",
      "batch loss:  0.749207\n",
      "batch loss:  0.74171\n",
      "batch loss:  0.630487\n",
      "batch loss:  0.75035\n",
      "batch loss:  0.814735\n",
      "batch loss:  0.678174\n",
      "batch loss:  0.653887\n",
      "batch loss:  0.742914\n",
      "batch loss:  0.622021\n",
      "batch loss:  0.757206\n",
      "batch loss:  0.773294\n",
      "batch loss:  0.581964\n",
      "batch loss:  0.638196\n",
      "batch loss:  0.647462\n",
      "batch loss:  0.555044\n",
      "batch loss:  0.660434\n",
      "batch loss:  0.658381\n",
      "batch loss:  0.638247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch loss:  0.591536\n",
      "batch loss:  0.590125\n",
      "batch loss:  0.673926\n",
      "batch loss:  0.562992\n",
      "batch loss:  0.63586\n",
      "batch loss:  0.644825\n",
      "batch loss:  0.724752\n",
      "batch loss:  0.548931\n",
      "batch loss:  0.593006\n",
      "batch loss:  0.615074\n",
      "batch loss:  0.657803\n",
      "batch loss:  0.651391\n",
      "batch loss:  0.557015\n",
      "batch loss:  0.541693\n",
      "batch loss:  0.605047\n",
      "batch loss:  0.606394\n",
      "batch loss:  0.721519\n",
      "batch loss:  0.667961\n",
      "batch loss:  0.663782\n",
      "batch loss:  0.703548\n",
      "batch loss:  0.648112\n",
      "batch loss:  0.70779\n",
      "batch loss:  0.597324\n",
      "batch loss:  0.579956\n",
      "batch loss:  0.503678\n",
      "batch loss:  0.548331\n",
      "batch loss:  0.621853\n",
      "batch loss:  0.554443\n",
      "batch loss:  0.558105\n",
      "batch loss:  0.480322\n",
      "batch loss:  0.582428\n",
      "batch loss:  0.680288\n",
      "batch loss:  0.562503\n",
      "batch loss:  0.578213\n",
      "batch loss:  0.465844\n",
      "batch loss:  0.664197\n",
      "batch loss:  0.566448\n",
      "batch loss:  0.535415\n",
      "batch loss:  0.57708\n",
      "batch loss:  0.630064\n",
      "batch loss:  0.628629\n",
      "batch loss:  0.563072\n",
      "batch loss:  0.586873\n",
      "batch loss:  0.51222\n",
      "batch loss:  0.625457\n",
      "batch loss:  0.498112\n",
      "batch loss:  0.542514\n",
      "batch loss:  0.581894\n",
      "batch loss:  0.60898\n",
      "batch loss:  0.450452\n",
      "batch loss:  0.550079\n",
      "batch loss:  0.548221\n",
      "batch loss:  0.437106\n",
      "batch loss:  0.551199\n",
      "batch loss:  0.578252\n",
      "batch loss:  0.604449\n",
      "batch loss:  0.607145\n",
      "batch loss:  0.488696\n",
      "batch loss:  0.494335\n",
      "batch loss:  0.51625\n",
      "batch loss:  0.524386\n",
      "batch loss:  0.634987\n",
      "batch loss:  0.504549\n",
      "batch loss:  0.525367\n",
      "batch loss:  0.46671\n",
      "batch loss:  0.500752\n",
      "batch loss:  0.439862\n",
      "batch loss:  0.445421\n",
      "batch loss:  0.477347\n",
      "batch loss:  0.435165\n",
      "batch loss:  0.557917\n",
      "batch loss:  0.396156\n",
      "batch loss:  0.36881\n",
      "batch loss:  0.279265\n",
      "batch loss:  0.464981\n",
      "batch loss:  0.453215\n",
      "batch loss:  0.489777\n",
      "batch loss:  0.61843\n",
      "batch loss:  0.463389\n",
      "batch loss:  0.41041\n",
      "batch loss:  0.48232\n",
      "batch loss:  0.498059\n",
      "batch loss:  0.470781\n",
      "batch loss:  0.478823\n",
      "batch loss:  0.438895\n",
      "batch loss:  0.49951\n",
      "eval_acc  0.8544\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(4):\n",
    "        print('epoch: ', epoch)\n",
    "        nb_batches = int(math.ceil(float(len(X_train)) / batch_size))\n",
    "        for batch in range(nb_batches):\n",
    "            start = batch*batch_size\n",
    "            end = min((batch+1)*batch_size, len(X_train))\n",
    "            \n",
    "            feed_dict = {x: X_train[start:end],\n",
    "                        y: Y_train[start:end]}\n",
    "            train_step.run(feed_dict=feed_dict)\n",
    "            myloss = loss.eval(feed_dict=feed_dict)\n",
    "            print(\"batch loss: \", myloss)\n",
    "            \n",
    "        nb_batches_eval = int(math.ceil(float(len(X_test)) / batch_size))\n",
    "        \n",
    "        accuracy = 0.0\n",
    "        for batch in range(nb_batches_eval):\n",
    "            start = batch*batch_size\n",
    "            end = min((batch+1)*batch_size, len(X_test))\n",
    "            \n",
    "            \n",
    "            feed_dict2 = {x: X_train[start:end],\n",
    "                        y: Y_train[start:end]}\n",
    "            curr_corr_preds = correct_preds.eval(feed_dict=feed_dict2)\n",
    "            accuracy += curr_corr_preds.sum()\n",
    "            \n",
    "        print(\"eval_acc \", accuracy/len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9728"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
